---
date: 2023-05-16
title: MOZ SERPs
permalink: /blog/moz-serps/
layout: plain
---
# MOZ SERPs



Alright, so we're MOZ and we've go a lot of data. And I've got to describe it
to market it. There's our external data as-is through our MOZ Links API and our
MOZ Pro product. I'm not going to go into that because... well, it's out there
and you can get it from there. But let's say you were some super-special
customer looking for advantages in training your AI models and you realize a
few things. First, you realize that a strong indication of how you should be
prioritizing an GPT-like giant-scale crawl of the Web should be based on whose
content is most effective. And how do you measure that?

Well, SERPs of course! That stands for Search Engine Result Positions, and it's
so much more than just search positions these days. That term is a holdover
from the days when Google was just about those Top-10 blue links on page 1 of
search results. And that would indeed be a good indication of whose content is
most effective on what keywords, because hey, Google thinks so, right?

So, SERPs at scale, a certain "corpus" of keywords that have been consistently
tracked over time, and somehow normalized enough to contend with geography and
platform issues (searching from where and from what) and you've got a pretty
good idea of who's getting what traffic on what keywords. There's a certain
zero-sum game, in the game theory sense popularized by John Nash in the movie
***A Beautiful Mind***, where the sum of all the traffic on a keyword is
constant, and the more you get, the less your competitors get. There are
winners and losers for any given time-period, and there are movers and shakers
over time. Our SERPs data is excellent for this, and we've got a lot of it.

However, the purely Top-10 blue links view of the world is long over, search
results now being a rich tapestry of images, videos, maps, news, and other
types of content. Rich Snippets, the Knowledge Graph, People Also Ask, and
other features are now the norm, and it's all shoved there in the "view-source"
HTML gobbledegook you can view from your browser after performing a search.
We've got lots of that. But the gobbledegook is not enough. It needs, in
addition to the aforementioned normalization, a lot of cleaning up. And it's no
longer just about the blue links, it's about the content of the page itself.

And so we parse all that view-source HTML into structured JSON data. Even
though we buy the raw SERPs from other sources, one of our completive
advantages is being in the "arms race" of parsing that data into all the
different things that appear and disappear in search over the years. For
example, if Knowledge Graph has its own "card" in the SERPs, we parse that out
and can give Name, Address and full contact information for the entity. Same
with Rich Snippets and other features. It's JSON, and thus not entirely easily
handled by the world at large who prefers Excel or SQL-style row and column
matrices. So we do that too, and we do it well and even layer in a value-add of
our own DIFFICULTY, VOLUME, OPPORTUNITY and POTENTIAL metrics, making it better
than competitors' pure extraction.


We call our JSON SERP data unparsed, although it's really quite well parsed by
us already into an entirely usable format for those adept in the ways of JSON
or using a system like MongoDB or Snowflake that specifically goes out of its
way to make it easy. There's value in that, such as our own Dr. Pete coming up
with new "opinionated" or "derivative" metrics. In the case of is new work,
Brand Authority, he looks at the "site links" that appear in the SERPs for a
given domain and uses that as a proxy for how much Google trusts that domain
for the keywords in the special sub-site links. It's a good idea, and customers
of our SERPs data may want the same abilities.

For those who want to do it a little more old-school, we do convert the JSON to
a SQL-style, Excel-style, CSV-style, or whatever-style row and column matrix.
It's much easier to directly query with everyday tools to for the common SEO or
developer. The fields are: LOCALE, ENGINE, KEYWORD, URL, DOMAIN, FQDN,
PROTOCOL, HOST, SUBFOLDERS, RANKING, DIFFICULTY, VOLUME, OPPORTUNITY, POTENTIAL
FILE_NAME. Again, four of our proprietary fields are layered in there, so even
our distilled down parsed data is more valuable than the raw data, and from
competitors who don't have such proprietary metrics. We also throw in some
goodies like subfolders and locale that could let you do some interesting
investigations. For example, you could see how a given domain is doing in
different countries, or how a given subfolder is doing in different countries.























