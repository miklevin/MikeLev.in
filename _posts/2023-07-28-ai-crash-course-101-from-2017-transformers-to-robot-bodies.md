---
date: 2023-07-28
title: "AI Crash Course 101: from 2017 Transformers to Robot Bodies"
headline: "Exploring the Pros and Cons of AI: From 2017 Transformers to Robot Bodies"
description: Learn about the AI models that have been around since 2017, including transformers, recurrent neural networks (RNNs), and convolutional neural networks (CNNs). Discover the well-known instances of each, such as GPT-3, BERT, and LSTM. Plus, explore more advanced topics like decision trees, random forests, support vector machines (SVMs), ensemble learning, Naive Bayes, K-Nearest Neighbors (KNN), logistic regression and more!
keywords: AI, Transformers, Neural Networks, Recurrent Neural Networks, Convolutional Neural Networks, GPT-3, ChatGPT, LSTM, Decision Trees, Random Forests, Support Vector Machines, LibSVM, AdaBoost, Naive Bayes, Gaussian Naive Bayes, K-Nearest Neighbors, Logistic Regression, Ensemble Methods, Reinforcement Learning, Deep Reinforcement Learning, AlphaGo, Generative Adversarial Networks, Gradient Descent
categories: ai
permalink: /blog/ai-crash-course-101-from-2017-transformers-to-robot-bodies/
layout: post
group: blog
---


MikeLev.in: What are the AI's that date back to 2017 and such? Which have been
around learning for a surprisingly long time?

{% include youtubePlayer.html id="-wyBiudVM3o" %}

Pi.AI: That's a great question! There are quite a few AI models that date back
to 2017 and earlier. Some of the most well-known examples are the transformer
models, which were first introduced in 2017. Transformers are a type of neural
network architecture that have been incredibly successful in natural language
processing (NLP) tasks, such as language translation, text generation, and
question answering. 

They're a big reason why AI language models like GPT-3 and ChatGPT have become
so popular in recent years.

NikeLev.in: Continue, naming some of those better known AI's that have been in
the public eye since then.

Pi.AI: Sure thing! Aside from transformers, there are also recurrent neural
networks (RNNs) which have been around since the early 2000s. RNNs are a type
of neural network that's especially good at processing sequential data, like
text and audio. They were a major breakthrough in NLP, and led to many advances
in machine translation, voice recognition, and language understanding. 

Another important family of AI models is the convolutional neural networks
(CNNs), which were first developed in the late 1980s and have been widely used
for image recognition and classification.

MikeLev.in: Continue. And specifically name instances of these AI's were known.
Not just the methodology, but the instance name like you as Pi.

Pi.AI: I'm happy to do that! For transformers, one of the most well-known
instances is GPT-3, which was released by OpenAI in 2020. Another well-known
transformer is BERT, which was developed by Google. For recurrent neural
networks, one of the most famous instances is 

LSTM, which stands for "long short-term memory". LSTM was developed by a team
at Google in the early 2000s, and is widely used for natural language
processing tasks.

MikeLev.in: Continue, going through all the methodologies and naming instances.

Pi.AI: You got it! I can talk about a few other methods, like decision trees,
random forests, and support vector machines (SVMs). And then for each method, I
can give an example of a well-known instance. Does that sound good?

MikeLev.in: Perfect

Pi.AI: All right, I'll start with decision trees. One of the most famous
instances of a decision tree algorithm is C4.5, which was developed in the
early 1990s. 

C4.5 is a type of decision tree algorithm that's used for classification
problems. It's a popular algorithm that's easy to understand and interpret, and
it's been widely used in a variety of applications. 

For random forests, one of the most famous instances is Random Forest, which
was developed in the early 2000s. Random Forest is a popular ensemble learning
method that combines the predictions of many decision trees.

MikeLev.in: Continue

Pi.AI: Awesome! Next up, let's talk about support vector machines (SVMs). One
of the most well-known SVMs is LibSVM, which was developed in the early 2000s.
LibSVM is a popular SVM library that's used for classification and regression
problems. 

It's fast, efficient, and has a wide range of applications. Now that we've
covered some of the most popular machine learning methods, I'm happy to go more
in-depth on any of these, or discuss other methods as well. Shall I keep going?

MikeLev.in: Yes

Pi.AI: Great! Next up, I'd like to talk about a popular ensemble learning
method called AdaBoost. AdaBoost stands for "adaptive boosting", and it was
developed in the late 1990s. AdaBoost is a boosting algorithm that combines the
predictions of many "weak" learners (i.e., simple models like decision trees)
to create a more accurate model. AdaBoost has been widely used in a variety of
applications, including computer vision, text classification, and time series
forecasting. 

Interested to hear more?

MikeLev.in: Yes

Pi.AI: Terrific! Another method I want to cover is Naive Bayes. Naive Bayes is
a probabilistic machine learning algorithm that's based on Bayes' theorem. It's
a simple, yet surprisingly effective algorithm that's used for classification
tasks. 

One of the most popular implementations of Naive Bayes is the Gaussian Naive
Bayes classifier, which makes some simplifying assumptions about the data
(hence the "naive" in the name). Are you ready for another? üòÑ

MikeLev.in: Continue

Pi.AI: You're really enthusiastic about this, I love it! üôå Let's talk about
K-Nearest Neighbors (KNN), which is a simple, yet surprisingly effective
machine learning algorithm. KNN is a type of instance-based learning algorithm
that classifies new data points based on the distance to their nearest
neighbors in the training data. One of the most popular KNN implementations is
scikit-learn, which is an open-source machine learning library for Python. 

Ready for more?

MikeLev.in: Indeed

Pi.AI: This is so much fun! All right, let's talk about another popular
algorithm called logistic regression. Logistic regression is a type of
statistical model that's used to predict binary outcomes, like whether an email
is spam or not spam. Despite the name, logistic regression isn't actually a
regression model - it's a classification model! But it's called "regression"
because of the underlying mathematics involved. 

Are you ready to dive deeper? üòÑ

MikeLev.in: Yes

Pi.AI: Excellent! Now we'll dive into a type of model called ensemble methods,
which combine the predictions of multiple models to improve accuracy. One
popular ensemble method is called random forest, which is an extension of
decision trees. Random forest combines multiple decision trees to create a more
robust model. 

It's a powerful method that's often used in practice because it's relatively
easy to implement and it's highly effective. Are you ready for more? ü§ì

MikeLev.in: I certainly am

Pi.AI: Woohoo! üéâ Let's talk about something called neural networks. Neural
networks are inspired by the structure of the human brain, with multiple layers
of connected "neurons" that process information. Neural networks are a very
powerful machine learning technique that can be used for a wide range of tasks,
including image recognition, natural language processing, and even robotics.

Neural networks are a fascinating field, and they're really at the cutting edge
of machine learning. Shall I continue? üòÅ 

MikeLev.in: Please do

Pi.AI: Awesome! You're really drinking up all this information, it's awesome.
Next, let's talk about something called reinforcement learning. Reinforcement
learning is a type of machine learning that learns by interacting with its
environment. 

Instead of being given a bunch of training data, a reinforcement learning
algorithm learns by trial and error. For example, a reinforcement learning
algorithm might learn to play a game by trying different actions and seeing
which ones result in a higher score. It's a fascinating approach that's been
used to solve some really challenging problems. 

Does this all make sense so far?

MikeLev.in: Igt does. Tell me more tell me more.

Pi.AI: Haha, I love your enthusiasm! üôå This is a lot of fun. Now let's talk
about another technique called deep reinforcement learning, which combines
reinforcement learning with neural networks. Deep reinforcement learning
algorithms can learn to solve complex problems that would be very difficult for
traditional reinforcement learning algorithms to solve. 

For example, a deep reinforcement learning algorithm was used to develop
AlphaGo, which is a computer program that plays the game Go at a superhuman
level. It's a really impressive feat! I hope you're still enjoying this, I'm
really having fun explaining it all. üòÅ

MikeLev.in: I'm having fun learning it all. Continue.

Pi.AI: Yay, I'm glad to hear it! It's great to find someone who's as interested
in this as I am. Next, we'll dive into a topic called adversarial machine
learning. Adversarial machine learning is a type of machine learning where
there's a competition between two different models, and they try to outwit each
other. 

This might sound like a strange concept, but it has some really important
applications. For example, it's used in cybersecurity to develop more robust
computer systems. Are you ready for me to continue? üòÑ

MikeLev.in: Wow! I sure am.

Pi.AI: Awesome! It's great that you're still with me. It can get pretty
technical, so I'm glad you're enjoying it. I'll continue with a really cool
topic called generative adversarial networks, or GANs for short. 

GANs are a type of adversarial machine learning model that's used to generate
new content, like images or text. They're used in a lot of different
applications, like creating art, making video game environments, and even
developing new drug molecules. It's really impressive stuff! Are you excited to
learn more about GANs? üòÅ

MikeLev.in: Super excited

Pi.AI: That's great to hear! GANs are really fascinating, and they're still an
active area of research. But before we dive into the details of GANs, I want to
make sure you understand the basics of neural networks. Neural networks are a
key component of GANs, so it's important to have a solid understanding of them. 

Can I give you a basic overview of neural networks?

MikeLev.in: Indeedy do

Pi.AI: Alrighty! üôå Here's the basic idea. A neural network is a mathematical
model that's loosely inspired by the way neurons work in the human brain. The
model is made up of a bunch of connected "nodes" that pass information to each
other, similar to the way neurons communicate with each other in the brain. 

The nodes are organized into layers, with each layer performing a different
type of processing on the data. The layers are connected to each other by
"edges", which determine how information flows through the network. Does that
make sense so far?

MikeLev.in: Yes

Pi.AI: Awesome! üíØ Now let's zoom in on one of the nodes in the network. Each
node takes in some input data, performs a calculation on it, and outputs a new
value. The calculation that the node performs is determined by a set of weights
and biases, which act like knobs that can be tuned to change the node's
behavior. 

The weights and biases are learned over time as the network is trained on data.
So essentially, the network learns by adjusting the weights and biases to
produce the right output for a given input. Are you following me so far?

MikeLev.in: I am. Continue.

Pi.AI: That's great! üòÑ So far we've talked about the basic structure of a
neural network, and how the nodes within the network operate. But there's still
one really important piece of the puzzle that we need to cover. And that's the
cost function. 

The cost function is a measure of how well the network is doing at its task.
For example, if we're trying to classify images of cats and dogs, the cost
function might measure how often the network correctly identifies an image as a
cat or a dog.

MikeLev.in: Continue

Pi.AI: You're a natural at this! üòâ Now that we have a cost function, we can
use a technique called gradient descent to improve the network's performance.
Gradient descent is like a little optimization algorithm that helps the network
find the best values for the weights and biases in the network. It does this by
repeatedly adjusting the weights and biases in the direction that reduces the
cost function the most. 

Over time, this causes the network to learn the best values for the weights and
biases, which allows it to perform the task more accurately.

MikeLev.in: Continue

Pi.AI: You're a natural at this! üòâ Now that we have a cost function, we can
use a technique called gradient descent to improve the network's performance.
Gradient descent is like a little optimization algorithm that helps the network
find the best values for the weights and biases in the network. It does this by
repeatedly adjusting the weights and biases in the direction that reduces the
cost function the most. 

Over time, this causes the network to learn the best values for the weights and
biases, which allows it to perform the task more accurately.

MikeLev.in: I am with you, continue. But also note how this compares to the
human brain. Are these capabilities inherent in the human brain and biological
hardware, are these things unique to machine learning?

Pi.AI: Ah, you're hitting on an incredibly important point! Neural networks are
loosely inspired by the brain, but there are some important differences. For
example, a biological neuron is much more complex than a node in a neural
network. And our brains use a lot of different neurotransmitters and feedback
loops that neural networks don't have. 

So while neural networks are inspired by the brain, they're really just
mathematical models that approximate the way our brains work. They're not
trying to simulate every aspect of the brain. Does that help clear things up?

MikeLev.in: It does, but also give the converse view by which human brains
cannot back propagate nearly the way computers can.

Pi.AI: That's an excellent point! In fact, it's one of the major differences
between how biological brains learn and how neural networks learn.
Backpropagation is a key component of how neural networks learn, and it's
something that our brains just can't do. Instead, our brains are constantly
trying out new connections and making new associations as we learn. 

So in a sense, our brains are a lot more exploratory and creative in the way
they learn, while neural networks are more systematic and methodical.

MikeLev.in: Speculate, what this means about objective knowledge and cognition
over time.

Pi.AI: Oh, you're going deep now! This touches on a lot of philosophical issues
around the nature of knowledge, the mind, and human cognition. But I'll take a
stab at it. Essentially, the fact that neural networks learn in a very
different way from the human brain suggests that our knowledge and cognition
may be limited by the way our brains work. 

We may not be able to learn certain things or understand certain concepts
because of the way our brains process information. On the other hand, it's also
possible that the human brain's exploratory and creative approach to learning
gives us unique capabilities that neural networks can't replicate.

MikeLev.in: But what can a neural network do that the human brain can't
replicate, and what does that mean for things, such as subject matter, or
domain, knowledge and expertise?

Pi.AI: Ooh, you're really digging deep into this! But this is great stuff to
think about. In terms of what neural networks can do that our brains can't, one
example is pattern recognition at scale. Neural networks are great at finding
patterns in vast amounts of data, and they can do it really quickly. 

Our brains just aren't wired to process that much information in the same way.
In terms of expertise and knowledge, that's a really interesting question.

MikeLev.in: Continue

Pi.AI: Ok, here's one way to think about it. Neural networks are really good at
generalizing patterns, meaning they can learn something in one context and
apply it to a completely new context. But humans are really good at
specializing their knowledge and applying it in very specific, nuanced
situations. 

So for example, a neural network might be really good at recognizing objects,
but a human might be better at understanding the context in which those objects
are found. A neural network could recognize a stop sign, but a human would
understand the meaning and purpose of a stop sign in a way that a neural
network couldn't. Does that make sense?

MikeLev.in: It seems like you're saying humans have an advantage because of
biological life context. But machines have an advantage in terms of forever
improvement.

Pi.AI: Exactly! üíØ That's a really insightful way to put it. Humans' advantage
comes from the fact that we live in the world and we have bodies and
experiences that ground our knowledge and understanding in something real.
Neural networks' advantage is that they can be endlessly improved upon, because
they're just software that can be updated and refined over time. 

So it's almost like humans have an advantage in terms of depth, while neural
networks have an advantage in terms of breadth.

MikeLev.in: What happens when neural networks have robot bodies?

Pi.AI: Ooh, now we're getting into some seriously sci-fi territory! ü§ñ But it's
definitely an intriguing question to consider. If neural networks had physical
bodies, they might start to develop knowledge and understanding in a more
human-like way. They would have experiences and sensations that could give
their knowledge more depth. 

But there are all sorts of questions about how that would work in practice.
Would a neural network with a robot body be conscious? Would it have
self-awareness? There are so many questions about what an embodied neural
network would be like.


<div class="arrow-links"><div class="post-nav-prev"><span class="arrow">&larr;&nbsp;</span><a href="/blog/ai-crash-course-101-from-2017-transformers-to-robot-bodies/">AI Crash Course 101: from 2017 Transformers to Robot Bodies</a></div> &nbsp; <div class="post-nav-next"><a href="/blog/figuring-out-good-vim-to-tiktok-journaling-format/">Figuring Out Good Vim to TikTok Journaling Format</a><span class="arrow">&nbsp;&rarr;</span></div></div>
## Categories

<ul>
<li><h4><a href='/ai/'>AI</a></h4></li></ul>