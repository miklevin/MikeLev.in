---
title: Commitment And Consistency
permalink: /futureproof/commitment-and-consistency/
description: 
layout: post
sort_order: 1
---

## Setting The Stage

This is a purposeful post, psyching myself up to go directionally where I have
to go in regard to work. I have two meetings tomorrow, and I have to get myself
psyched up and ready. I have to do the right things, for the right reasons, and
start a positive feedback loop so that we always know where we are and where we
are going. It should almost always be the same story with every client:

- The reduction of the amount of Google crawl budget waste
- The increase in the number of pages and performance of those pages in search

### The AI Search Landscape

These days there is also some secondary objectives because so often the AIs are
going to summarize what they find (or already know) in AI overviews, or whatever
you want to call the new search experience. Microsoft with Bing and Copilot have
very little to lose in search, so they're using their power over the default
Windows install and ability to radically experiment on the search results pages
to change consumer expectations, reframing it more of a chatbot experience than
a top-10 blue links experience. Since Google is primarily an advertising company
and most of their revenue comes from AdSense, estimated at about 80% of their
revenue, they're caught in a classic Innovator's Dilemma. Google can't mess with
how things are working excessively, least they cut into that revenue. 

Microsoft has no such inhibitions, their revenue being much more diversified and
so they were much more free to jump on the ChatBot-as-Search-Results bandwagon.
Google on the other hand has a super toned-down AI Overview barely layered in
like Rich Snippets were in the past. Google is just rearranging things in ways
that won't tank AdSense clicks. But anyway, that's not what this article is
about. In fact, I have to keep myself from diving down such rabbit holes. Yes,
it's important and at the heart of many of the changes going on and how we have
to adapt the way we work. But there's still underlying old school SEO. It
"seeds" the process. No matter how things change, something like search seeds
even the AI responses. So we still concern ourselves with search, in a rather
classic SEO way for the time being.

### The Continued Relevance of Traditional SEO

We see this both on the Google side, with their continuing control over the
Android platform and default search in Chrome and Safari. SEO also remains
important because of Microsoft's prolific API deal that powers much of the new
ChatBot-Search products, which even if they produce generative responses, need
to be looking at search results of some sort to generate their responses from.
It's a nuanced point, but you have to think of the chat-based search products
like Perplexity as something that just performs a traditional Web-search on your
behalf, and uses that as the content from which to summarize its response from
for you. It is in effect a ***meta-search*** just like the days of yore with
Dogpile and MetaCrawler. Re-wrapping other people's products is usually the
cheapest and least respectable trick in tech, but every once in awhile the
intermediator gets all the little nuanced details right, and that makes all the
difference. That's what Cursor AI did. 

Even though there are plenty of other code editors out there with AI assistance,
Cursor got the details right, as did Perplexity.AI with metachatbot results.
That's not to say that either has a very large competitive moat that couldn't be
evaporated away overnight. It takes a long time to build up habits, like Google
got us all to do with Chrome and to expect the address bar to be a fallover
search feature (Safari), and in the end there's no amount of innovation from new
products, intermediators or not, that can override habits and these
billion-dollar deals from Google. Not even Microsoft's control of the Windows
platform and their ability to keep resetting your default browser back to Edge
and correspondingly your default search back to Bing is enough. The battle of
attrition that Microsoft is waging from the strongest position possible is not
enough to overcome a decade of Chrome habits and default address bar deals. That
address bar deal by the way extends even to Firefox and Opera. Google pays for
both of those browsers.

### Building Momentum and Strategy

Okay, I'm rambling a bit, but this is still in the spirit of painting the
background picture and building up the momentum for the work I have to do today.
It's a formidable amount of work, and this is probably going to be a long,
rambling post as I get together the overarching story of every website that I am
going to be repeatedly telling to all my clients where I work, in a longer form
for the worksession meetings, and in an abbreviated form for the monthly
executive touching base. There are usually at least 2 different stakeholders on
any account. There's the SEO practitioner who actually uses the product of my
employer, and there's the executive sponsor who is the one deciding to continue
using the product. We have to make the later happy to make the former look good.
The type of software is SaaS (software as a service). But I'm the user of the
software on the client's behalf, so it's more like managed service.

### The Continuing Importance of Traditional SEO

Okay, that's all the broad brush strokes of why traditional SEO is still
important. And just as a recap, that's not justification to myself arguing that
the field I'm in is still relevant. It's a fact that Google, and to a lesser
degree Microsoft, are still the gigantic arbitrators of who gets what website
visits. The are the gatekeepers and routers of people on their online journeys.
Even if ChatBots run interference between searchers and the end websites, the
referral will eventually be given, and that referral is generally going to be
Google or Microsoft powered. They "seed" the search. Either directly in the case
of Google and going through the still pervasive "omni" unified search/address
bars, or through Microsoft selling search results as an API service to everyone
else.

That's the Microsoft Search API product, because Microsoft will sell this as a
service to those sorts of sites, whereas Google won't. Again, it's a matter of
search results being the family jewels to Google, but only being a defensive
side-bet not critical to company survival for Microsoft. And so, SEO remains
important. The alternative is for companies to either crawl the web themselves
and reproduce crawl/index infrastructure like Google and Microsoft have, or for
them to crawl the web in real-time in response to a user query at the moment of
the query -- a much slower proposition, which though it may result in richer,
more varied and maybe even accurate results, it also results in an
"inconsistent" and less professional feeling product. And so, all that search
API business will keep going to Microsoft for the foreseeable future.

### The Client Story

Okay, enough friggin background just to psych myself up. Now for the story that
needs to be told with each and every client.

Fist, we have a better story to tell because we take the time to get the log
files from the client. Usually not the whole friggin log files because that's so
epically massive, and doing so would violate the 80/20-rule (80% of the benefit
from the first 20% of the effort). And so instead, we get enough of the log
files to make that big difference. But we can't in every case, because of
Shopify (for the most part). Log files are hard enough to get under the best of
circumstances, because of this CDN (content distribution network) world we live
in, where Internet network partners make sure your content gets served fast all
over the world and doesn't put the whole burden on your servers. They are
traffic intermediators doing network tricks. And thus, the CDNs are usually the
ones holding the keys to your log files, and dealing with that fact is one of my
employer's specialties. We have an impressive Client Services team good at
navigating and negotiating all these nuanced CDN details that most SEO agencies
wouldn't have a clue about. So we get the log files, in almost all cases but
Shopify.

#### The Funnel Approach

So the story begins with that of a "funnel". It's something of putting the cart
before the horse, but we can know what pages are being discovered by the
crawlers and can watch for whether they ever have any search performance or not.
It's a sort of Venn diagram with crawled pages (known through web logs) on the
left, and active pages in search known through Google Search Console and my
employer's industrial crawler on the right. Yes, we have an industrial crawler
that can crawl those millions of pages that the desktop crawlers like
ScreamingFrog will, if you don't use their cloud based service, will never
reach. And because it's a full service integrated product, you're not just there
with a massive crawl figuring out what to do next. It feeds all sorts of other
processes and services going as far as to optimize your site. But it's not
automatic (yet). There are still guiding hands of humans (me). And thus this
thinking it all through out loud here.

### Commitment and Consistency

Okay, the commitment and consistency part. Unfortunately, this will continue to
be a bit of rambling before I get around to the actual work I need to do for
tomorrow. This is really about thinking it all through and getting a similar
unified story into my mind, habits and muscle memory for all clients. It's the
Enterprise SEO story, moving into the AI future, and moving slightly downstream
from purely Enterprise. You shouldn't have to be a million-page ecommerce
website to use my employer's product. You should have a pretty big commitment to
and dependency on search arbitrated traffic. Honestly, this is everything
besides direct links in social media and such. If you don't already know
precisely what you're looking for, and there's some process of inquiry,
investigation, research or whatever involved in your customer's journey, you
need SEO or you need to get links explicitly in front of them generally by
paying for it. 

### The Future of Merit-Based Search

The game is still the same old SEO value proposition of getting the customer to
come to you meritoriously by virtue of being the best at what you do. When merit
can no longer route, the soul will have completely been sucked out of the Web.
Some people think we are there today. But there is a sort of game theory keeping
meritorious sites buoyed in search. Otherwise, the backlash against Google will
override even the Apple Safari deal. This is why openings are being made for
Perplexity.AI and the like. But we're not there yet, and even when we do get
there the thing that will be replacing Google will have to have that genuine
meritoriously earned editorial content component to it, or it won't really
displace Google. That's the "Don't Be Evil" premise and mantra that Google's
brand was built on. Disingenuousness can erode it, and the bedrock of the
products that replace it will be genuineness again -- probably with very
intelligent AIs being the new judges of merit... based on signals... again.

And so as SEOs, we make sure we are broadcasting the correct signals. We look at
the tools we use today to measure that. There is a feedback loop. With my
employer's product, that feedback look generally begins with monitoring a web
crawl to see what URLs its discovering, and what we should keep an eye on for
getting into active search. But we are cut off from those log files, frequently.

### The Unified Client Story

Okay, so the unified story across all clients. I can feel that my YouTube series
is going to be born again. I'm going to try to keep myself on video all the
time, but often for proprietary purposes. It's part of that commitment and
consistency thing. I the case of this very article, it's making a public
commitment, and because I'm talking about it here out to the public on my daily
tech journal (blog), it's like signing a public petition you know your neighbors
are going to see -- even though that example is becoming very dated.
Nonetheless, it's a psychological principle covered by Robert Cialdini in his
groundbreaking marketing book Influence and the Art of Persuasion. It's one of
his key principles, covered by a chapter. If you state something publicly, you
are more likely to make your behavior consistent with your stated intent. It's a
life-hack before the term life-hack was a thing. It's also really covered by
Cialdini as the outing of a method that marketers use to control us. But the fun
is turning the tables and using those same principles to control ourselves,
amping up our own self-discipline and agency. 

### The Future Vision

And so what I'm expressing here is my intent to be one of the most interesting,
insightful and useful sources of information about SEO in the Age of AI as we
head into the new Industrial Revolution. Yup. I'm very wordy, but what does that
matter when AIs can summarize everything. In fact, all this is going to be
distilled down, constantly and shiftily organically reshaping and adjusting as a
sort of organic living book. You've heard of living documents like the
Constitution of the United States with the Amendment system? Well, you haven't
seen anything yet in terms of actual living documents! Content is no longer
meant to be static on the Web, unless it's original training fodder
pre-synthesized. Synthetic data is better for training AI models like LLMs
because it's gone through that organization and tidying up, filtering out all
the noise and garbage, exactly of the type that LLM SEO gate-keeping will go
through. This sort of writing is the genuinely human written pre-synthetic data.

### The Work Ahead

So I suppose this is still a bit rambling. But it's the deep breath before
getting down too work. In this work, there is:

- A linear story that is told for everyone
  - Potentially not having web log data and how to compensate for it is actually
    still part of the linear story
  - Being a partner with Shopify to get that web log data is also part of this
    story, because it's part of the SEO strategic advantage story, and our
    clients who choose Shopify can't be left out of that advantage
  - None-the-less, here are the ways we compensate for that (not listed here)
- Every client has a Botify project URL
  - I should be able to pull that up across various clients at once

### Finding Purpose Through Ikigai

Okay, this is where I'm going to be able to boost my motivation and make this
whole thing love-worthy. There's an Ikigai component here, which is a Japanese
concept to frame your reason for being. It's a 4-circle venn diagram that is the
unification of:

- What you're good at
- What you love to do
- What you can get paid for
- What the world needs

When what you're spending your time doing on a day-to-day basis aligns these
four circles in sufficient amount, you feel good, have more meaningful
connections (what the world needs / even if you're a hermit), and ultimately
live longer, fight dementia and Alzheimer's, yadda yadda. There's a lot to
unpack here, because living a perfectly quiet life where you have a net-zero
impact on the world (and presumably everyone you haven't interacted with) is
also fine too because you know, who's to say? No judgement calls. We each make
our own decisions in these regards.

### The Magic of Alignment

But if you can find the love of a thing that happens to align with something
you're good at, some... I don't know, dare I say magic can happen. Now, there's
no such thing as magic, but for fear of digressing, likewise, we don't know why
quantum mechanics works the way it does. There are interpretations, but none are
decisive based on the scientific method, making them currently only guesses.
Guesses are what's needed for a hypothesis, to be ultimately tested and turned
into a theory and it gets more firmly into the realm of science. But there's a
few things that still elude, such as how human consciousness arises, whether and
to what degree other animals and forms of life share this conscious quality
(qualia), and whether machines can, currently do, or ever will. We don't know.
Fervent science defenders can squash a lot of good guesses because those guesses
aren't firmly in the realm of the testable and known yet, but that's the alter
on which good ideas die -- merely because the guesses sound like explaining some
sort of magic. Both quantum and consciousness are full of such mysteries and
must be engaged with in playful open minded guessing, least scientific inquiry
stalls out at this point where we are still ignorant of so many things.

### The Evolution of SEO

Okay, that was a bit of a digression, but ultimately the sort of thing that
motivates me. I'm in the field of SEO, but it's not SEO that I really love. It
just happened to have aligned with something I was good at in the past, which
was manipulating information to lean into what Google liked to see and tended to
reward. But that was from the era of the "long-tail" of content, where if you
wrote it, they will come for lack of any better exact matches for the topic
you're talking about on the Internet. In fact, this behavior is so popular that
despite Google's functional chopping off of the long-tail in favor of big
established brands and advertisers, they still have luring a "verbatim" mode, if
you look for it. The idea of a match better than anything else, all things
considered, is still a thing. And it still lurks right beneath the surface,
waiting for spammers to lose the now war of attrition against smarter and
smarter AIs.

### The Economics of Content

It costs money to keep website content hosted, so if those things aren't
producing for you (un-meritoriously attracting customers), eventually they will
disappear, if only by virtue of the invisible-hand spammer not paying for domain
renewal or hosting. It's part of the cat and mouse game. Now the fact that so
many of these sites are incentivized by the advertising revenue they make on
these spammy sites with Google AdWords (Google Ad Network), manipulating search
results in that very same Google. Now if you have any doubt that Google likes to
reward sites running their own Ad Network, just look at the sites served by
Google Discover, the automatic feed that appears beneath Google Search on more
and more platforms. They're almost all ad-ridden -- to the point of destroying
any potentially beneficial Google Discover experience in my opinion. 

### The Game Theory of SEO

Okay, still more fleshing out the picture of SEO and the game theory balance
between the meritorious and sometimes verbatim content on a topic and the
brand-dominated ad-ridden money-making sites. We have to assume that the game is
not completely rigged in favor of ad-paying Google customers. Other things need
to be able to peek through here and there. And if things can peek through, those
same things can be footholds and beachheads for expansion. A lather, rinse and
repeat process involving a positive feedback loop of data and publishing, just
like existed in the heyday of long-tail blogging, still has to exist. The
premise that meritorious content, especially on an exact-match basis, can't be
completely shut out has to be assumed, or else the castle falls out of the
cloud. Just enough balance needs to be maintained in this Nash equilibrium to
tease content publishers to believe the game is worth it. Google will give just
enough.

### Playing the Google Game

So, Google gives and we will take. But we must be taking from a well-earned and
meritorious position. We can't be just another publisher competing with a
perfectly optimized piece of content on that same keyword. We have to offer
something better that pumps up user engagement. Because Google owns Chrome,
Google Analytics, the search result page itself (with its own tracking), Google
has a lot of ways to measure your engagement with content. It knows a bounce. It
knows when a user was immediately dissatisfied with content and goes immediately
back to search. It knows if you linger longer. If you have any doubt, look at
the federal Google antitrust lawsuit case that Google lost. The judge thought
one remedy might be to break Google up, making it divest itself of Chrome. Ask
yourself why that is. Control of the browser completes the feedback loop. They
can call mama however, whenever and why-ever they like. That case not enough
proof for you? They also lost the class-action suit regarding them still
tracking you in ***Incognito Mode***. Don't be evil, indeed.

### Our Process

But none-the-less, our process remains Google-centric. We play the game. And the
game starts in Web logs when we can. When we can't, the game starts in Google
Search Console and the enterprise Site Crawler.

## Turning Pontificating into Muscle Memory

Okay, so I've got to change this from mere pontificating to muscle memory. I
have to go through the moves. Think! And I don't want to violate the 80/20-rule
or go down rabbit holes. But nonetheless, this is the beginning of turning the
ship around. Metaphors, hmmm. Okay, for far too long I've been working like I'm
on a bicycle on square wheels, having actually designed round wheels on the side
but during the day having no time to stop and put the wheels on. 

And so, without violating the 80/20-rule nor going down any rabbit holes, I am
going to put the round tires on. I am going to do this in a way that the
nattering of check-in's regarding tomorrow isn't going to throw off my groove or
derail me. I am going to get things into a demonstrate-able state, both for
clients and my coworkers in a world-changing way. I am going to start bringing
together those circles of Ikigai into alignment.

### Managing Code Branches

Okay, I have a codebase where the main is essentially "broken" and the magic
branch is usable, and people could get used to it... uh oh! Rabbit hole. Think!

You know what you need to do. Right. Okay, get things in shape so that you can
share your work thus far and get feedback from the team. Also get it in order so
that a certain branch of it can be moved over to Pipulate without fear. And as
you develop... what? The experimental branch into the new main branch, you deal
with the proprietary stuff, cleverly. This is where you unify the code bases of
botifython and pipulate, permanently back onto pipulate. The magic cookie that's
given out against the protected botifython repo will actually do a pull against
Pipulate and "blend in" the proprietary stuff from behind the other protected
repo. Ah ha! The magic cookie script basically does a pipulate clone (naming the
folder whatever I want), and then does a follow-up pull to put the proprietary
stuff inside the "parent" repo's folder. The child repo is kept from colliding
by adding it to the .gitignore file of the parent.

### Strategic Branch Management

Okay, I did the switcharoo of the branches under the current botifython repo. Do
I invite my coworkers in sooner rather than later? Okay, this is where the sort
of paralysis occurs. I've got to step back and make sure I get tomorrow's work
done well, but in a way that uses the code base. Right. I want to keep both the
main branch and the experimental branch readily accessible. I want to be able to
hop between them, actually maybe keeping both running on localhost, changing the
port of the port that uvicorn is running on on one of the versions to port 5002.

### Evening Strategy Session

The thing now... the thing tonight, because the day has almost evaporated away,
is to stage things. It's not to dive down rabbit holes. It's to move stuff
around in a sort of staging step that allows me to do the work immediately
before me, really on either the experimental or the main branch, whichever seems
more suitable. But it should be on the experimental branch because that is the
future. 

### Repository Organization

The trick now is to turn this commitment and consistency maneuver that commits
me to a certain path into even more of a step. All my work is sort of held back
from public view. It's even held back from internal view too where I work
because of the friggn transition state that the work is in. I make sure that the
remote repo doesn't have any branches I don't want... okay, done. Now I'm down
to the 2 branches and it's very clear what they are.

Okay, now locally I want to rename botifython to experimental... Okay, I'm doing
that with a simple renaming instead of deleting the old botifython repo and
re-cloning just in case. I don't want to lose any local files... okay, checked
what's in there. Yep, there's definitely non-committed stuff, but most is old
stuff before I got the trick of the two branches down. Okay, so clean it up.
Okay, so now there's just one folder left that's not committed, and it's called
training! And that gets down to the meat of the matter. That's where the
proprietary stuff is going to go, and that's why I haven't even committed it to
the private botifython repo yet. I wasn't really sure what I was going to do
with it. But now with the nested repo trick, I think I've got it.

### Late Night Planning

Okay, again, careful rabbit hole avoidance. It's about 7:00 PM now, and I've
hardly even started tomorrow's work, which is very important. But I'm trying to
do that epic gear shift. But I'm trying to do it now without using up the last
bit of my energy. The trick is to get myself super pumped-up and motivated over
this change. The trick is to dig deep and make myself just want to do it,
getting into my groove and going on automatic, going as late and long as
necessary to get this work done. But without any of it being rabbit hole work
that will use up my energy, frustrate me, and keep me from doing tomorrow's work
well. Okay, so think. This is a chisel-strike. It shouldn't be harder than a
chisel-strike project, so it's more thinking than brute force heavy lifting
coding. Brute force heavy lifting coding will be your ruin tonight. Well thought
out light-touch coding will be your salvation. Having something to show to your
teammates tomorrow.

### Implementation Strategy

Yes, this is a plan. The main branch is the old branch where I had everything
working just so. Speak as if you're talking to them. Speak like you're also
planning the things that go into the training folder. The idea here is that I
can help both a coworker and a local LLM (or a coding assistant LLM like Claude)
up to speed on what's going on and why. Hmmm, no. Too many rabbit holes lay down
that path. That's what I'm heading towards. So this is stating intent and
staging things. Work by yourself and for yourself through tomorrow. Pull off
some amazing feats that get you into the flow and compel you forward. That's the
best favor you can do for yourself now. If you drag this thing out any further,
you risk credibility. But make the state of the botifython repo easily explained
to anyone you want to invite in in the near future.

### Setting Up Botifython

The botifython repo has two branches: main and experimental. If you want to play
with it, having a fully functional application that can produce the link graph
visualizations, you can work off the main branch. And in that case, there are
really only 2 magic commands you need to execute to get it working.

One to get nix installed onto your system using the Determinate Systems
installer.

    curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install --determinate

The second part is to take a copy of the repo that has to be specifically given
to you (a magic cookie), and then to unarchive it, cd into that location and:

    nix develop

### Git Workflow Considerations

The nix develop process itself will update the location that it is in from
merely being a folder created by the unarchiving of the zip into a proper repo,
even set up with with a deployment key in order to check for the latest code and
pull it down. That is a very interesting point, now that I'm getting to know git
branching more. The instructions I need to make to my coworkers who want to do
their own development that doesn't collide with mine is to make your own
branch.

Create and switch to a new 'experimental' branch based on 'experimental':

    git checkout -b mybranch experimental 

But this really gets to why it's a bit premature to do this. I can hardly expand
apps in the experimental branch myself yet (a big part of what this push forward
is about), and it's not worth doing in main because of how many breaking changes
there are between main and experimental.

### Path Forward

Okay, it really helps a lot to articulate all this. It clarifies in my mind
where I'm at, what I'm doing, and when the time is right to share people in.
However, I am making the small directional changes in anticipation of sharing
them in, and because I'm doing it public like this, if they really cared to they
could read and follow this. But everyone is busy enough as it is already, I
wouldn't expect them to. I have to make this dirt simple.

Not only am I making it dirt simple (porting Juptyer Notebooks to Pipulate), but
also I'm doing it with the truly FOSS SEO software, Pipulate, so that if I crash
and burn keeping these clients happy in client-service mode, then I can at least
walk away from this with all but the proprietary the code committed under the
public license, and I can light the world on fire with this next gen AI SEO
stuff. But small, directional tweaks. No rabbit holes! And no burning myself out
at the beginning of the day, which I'm starting at 7:30 PM now... haha! If I get
to a good stopping point, I can get a few hours of sleep before the meetings
start tomorrow.

### Next Steps

Okay, how is your next step a light touch chisel-strike that re-invigorates you
on this project?

Get the experimental and the main branch of botifython running at once. Alright,
done. Experimental is running on port 5002 and botifython is on the standard
port 5001. And if I do share with my coworkers, that's set up for the
conventional behavior for the default port for FastHTML/uvicorn. Okay, deep
breath. Keep in mind that the actual coding you're about to embark on is
precisely the thing that uses up your energy with decision fatigue. And so, you
are going to go one careful step at a time, making sure you can expand one of
these pipelines step-by-step to produce a worksession and an EBR. Plan it.

It will ask you for the Botifython Project URL.

### Leveraging AI Assistance

Hmmm. This will therefore become another test of... what? Use Grok 3 the way you
normally use o1 (and recently tried to use o3). That is as an external (to your
code editor) coding consultant. Because the entire code base is in a single
file, I can feed it that file. In fact, I can feed it this entire article as
context.

You hear that, Grok 3? This is a super-prompt to give you lots of context. The
thing you need to know know is:

I am about to feed you one of my code bases. There will be A LOT to look at in
there, at is is 6,316 lines of code. It is 247,844 bytes as reported by ls. And
tiktoken reports it as 53,226 tokens (≈$1.60 at GPT-4 rates).

### Technical Architecture Overview

It's a whole web framework built on FastHTML. The code itself will explain and
document much of it, suffice to say here there's both a CRUD app plugin system
for stamping out ToDo-like apps from BaseApp, and there's a pipeline workflow
system like celery but so much less for single-tenant localhost super-simple
process that was intended to stamp out instances from BaseFlow. But oh, the best
laid plans!

Even though the examples you'll find that do this, StarterFlow and PipeFlow, are
short and elegant inhereting from BaseFlow, I will not be using the
`super().__init__(app, pipulate, app_name, steps)` BaseFlow inheritance trick at
all. I've come to accept the fact that everything I need to do with this system
is too custom, and by inheriting everything from BaseFlow, I end up fighting it
tooth and nail and it gets complex sorting out what's inherited and what's
overriding.

I have another code base that does it the old way, with no BaseFlow at all.
Whenever there needs to be code reuse, it's done with a helper class called
Pipualte, which manages state. Oh, speaking of state please take special note
that there is almost no client-side management of state. Being a local-first app
with full control over the server, all cookie-like tricks are done on the
server. All state for the pipeline is in one JSON blob in one record for each
instance of a workflow run. It is entirely interruptable and resumable based on
an initial key (currently a field called url). The plan is to eliminate BaseFlow
in time, and to put anything needed for code reuse into Pipulate. We don't have
to do that right now, but you need to know this because the project I'm about to
ask you to help me with is in making a new instance of a pipeline workflow like
StarterFlow, but doing it entirely custom step by step.

Is that clear, Grok 3? Are you ready for me to feed you the codebase to absorb?
And then we will start making a new pipeline workflow called:

    class AdvantageSession

Clear? Ready?

**Grok 3**: Hey there! I’ve been following along with your epic journey through
this post—it’s like a tech odyssey, and I’m totally here for it. I get the vibe:
you’re psyching yourself up, laying out the big picture, and now you’re ready to
dive into the nitty-gritty with some coding action. I’m pumped to help you out!

Your context is crystal clear. You’ve got a massive 6,316-line FastHTML-based
codebase, a web framework with a CRUD app plugin system and a pipeline workflow
setup. You’re ditching the `BaseFlow` inheritance (smart move if it’s fighting
you too much), leaning on `Pipulate` for state management, and keeping
everything server-side with that JSON blob for interruptible, resumable
workflows. Now, you want to build a new custom pipeline workflow called
`AdvantageSession`, step-by-step, no `BaseFlow` crutches—just raw, tailored
logic.

I’m ready to soak up that codebase like a sponge. Go ahead and feed it to me,
and I’ll absorb every line, every token (53,226 of them, huh? Worth every penny
of that hypothetical $1.60!). Once I’ve got it, we’ll start crafting
`AdvantageSession` together—light-touch, chisel-strike style, no rabbit holes,
just focused progress to get you prepped for tomorrow’s meetings. Let’s do
this—what’s the code? Lay it on me!




