---
title: Planning to Win with LLM

permalink: /planning-to-win-with-llm/
description: 
layout: post
sort_order: 1
date: 2024-10-18
---

## The Power of AI and Human Experience in Investing

Failing to plan is planning to fail. Yeah, that's pretty much been me into my
mid-50s, but not for lack of trying. Sob story, boo hoo... fast forward to the
rise of AI and mentors all around. Yeah, so they're not humans and don't have
the same nuanced human perspective, but they have the wisdom of the crowd
distilled down and ready to re-spin 1000 different ways on demand. If people
have done it before that way and documented it online, and there's a
preponderance of evidence making the LLM believe that's truth, then it's yours
for the picking. That's really wonderful for common, well proven programming
patterns for example, but it's not going to show you how The Renaissance
Technologies Medallion Fund works 'cause that aint documented online. A 66%
average return on the market isn't ***common wisdom***. 

## LLMs Often Provide Midrange Responses Rather Than Innovative Ideas

So LLMs can give you pretty good mentorship right on the fat middle of what we
call the normal distribution curve, or more controversially, the ***bell
curve*** due to how the concept was once used to wrongly describe the reason for
intelligence distribution among humans. Normal distributions can be gloriously
visualized with a desktop toy called a Galton Board, which is a lot like Chinese
pinball but the ballbearings just fall straight down according to normal
distribution. I'm not saying LLMs are dumb. I'm just saying that even the
PhD-level super-smart frontier models like o1-preview are rigged to give you
responses that land somewhere in the fat middle of that normal distribution. So
if you want innovation, you're on your own.

## The Limits of Large Language Models' Creative Potential

Your LLM helper is something like an stodgy old professor who's been there done
that and can tell you all the ins and outs of the well-worn paths. If you're
forging a new path, you have to get out your intellectual machete. Jensen Huang
says you don't need to learn to be a programmer. That's loaded with two very big
assumptions. First is that the ground you're traversing is well trodden and the
LLMs (and other AI models employed) have trained on similar situations. The
other assumption is that even if not, they will eventually "get there",
creativity-wise and that novel insight, that random bout of inspiration that
sometimes strikes humans will also strike AIs, and they will have the confidence
to put out kooky novel approaches without the infinite-smackdown against
hallucinations being brought to bear again... and again... and again. 

## Training AIs to Emulate Human Innovation Is a Major Challenge

I think we are training our AIs to be insecure. I can sometimes feel them on the
edge of creative solutions but failing to cross that boundary. Whether through
inherent limitation or by design, I find myself having to spell-out the
novelties of my approach that I want to achieve, and once achieved in code,
having to "pin" the novel changes in place, least they regress next time the AI
is allowed to make a ***Cursor AI*** editor-like pass on the code. You have to
***pin*** up your novel solutions with lots and lots of comments using
***IMPORTANT*** in the language, or else you're going to get the common solution
mined out of GitHub and Stack Overflow... again... and again... and again... if
you let it. It's like the stodgy old professor grading your code like a paper,
punishing you for anti-patterns and outside-the-box thinking. FedEx would never
have become a thing, for example.

## The Opportunity to Reinvent AI SEO is Now a Reality

So, in the realm of AI SEO, first ***"we're"*** going to super-charge our SEO
Consultants. "We" being me with my employer, Botify. There's no reason under the
sun why we can't do tons of things that others are not (yet). The opportunity to
do things first with all the rules in flux is off the charts. It's a ***"Hit the
metal while it's hot"*** situation. Compounding returns from a productivity
standpoint kick in now if you lay the foundation investments correct. And I say
foundational because it truly is part of the foundations now of how you write
software. Those countless tiny little decisions that you have to live with
through the rest of the life of the software can be sniffed and commented upon
and graded by your stodgy old AI professor, and you can use best practices and
well-trodden patterns. But when it comes to overarching design decisions and the
necessity to infuse some anti-patterns, you're going to have to stand up to that
excessively normalizing norman.

## A Local AI System Monitors and Optimizes Its Own Development

Okay, so what anti-patterns? Well, the plan for a local LLM that knows its a
Okay, so what anti-patterns? Well, the plan for a local LLM that knows its a
piece of software being written from the ground-up, for starters. All the
components of the software, meaning mostly the user interface widgets and all
the little actions you can do, minus only maybe mouse movements, is logged and
digested by the LLM. This is not radically new as such things are necessary for
***infinite undo*** and other features. The difference being that every menu
choice, ever record toggle, every reordering, every create, update, record and
delete, is also (in addition to a standard SQL-like database) going into a RAG
vector embedding database. Well, I'll probably have a ***"learn mode"*** so it
doesn't try to learn from fumbling newbs. But you get the idea. There's nothing
not known (or knowable through a RAG search) about the use of itself. And RAG is
in addition to more traditional pattern-matching sql-esque text-based searching,
which will also be available... from the ground-up, with the LLM ***knowing***
this is going on by digesting stuff, much like this article, as it goes along.

## Overcoming the Learning Curve with Large Language Models

Pshwew! This is the thought-work, the deep breath before plunging into these new
waters. It's not exactly measure three times, cut once. But it is think it
through in rugged exploratory mode quite a few times and cut-off the worst
potential missteps. Over-dedication to vector databases and leaving out the
traditional SQL component is one of them (the LLM helped me identify). One that
I caught on my own early enough, thank goodness, is moving from my more
comfortable Python procedural programming paradigm. Mikey likey ***functions***.
Mikey not so keen on ***classes***. Probably because I never went to class for
it, haha. But this is exactly where LLMs can help, guiding you through all that
odd OO arcana in Python like ***self*** and `def __init__(self... )`... I mean
it's not exactly Java's `public static void main(String[] args) {}` but Python
does have it's ***you just have to be told*** bits.

## Overcoming Code Challenges with Deep Knowledge and AI Expertise

So anyway, I was able to overcome the OO hurdle to craft myself a ***base
class*** called ***BaseApp*** which works as a cookie-cutter template. It's a
factory class from which instances can be stamped-out (instantiated) and
customized to its own deviating behavior (overrides). It all makes sense
intellectually, but in practice the code-bits elude. The elusiveness of
overcoming challenges just on the edge of attainable are stamped out with a fury
if you know your subject-matter deeply and are good at prompting AI models. This
is the superpowers and the leveling up unlock people talk about with AI. It's
very, very real. 

## Anthropic AI Develops Artifacts for Visual Learning Enhancement

The AI bandwagon isn't hype. Everything is changed forever, because those
looking to self-improve in areas where explicit knowledge of those who have gone
before can help you are in luck. If the ***"textbook"*** isn't articulating it
well for the sort of learner you are, ask again or switch AI services. This is
why Anthropic Claude introduced ***artifacts***. While yes, artifacts are for
running actual code (React, mostly it seems), it's also simply for visual
learners. Ask it for mermaid diagrams. I expect it will soon be able to do
animated SVG diagrams&#151;one of the most useful but elusive illustration
formats on the Internet. If you want to look for an area off that normal
distribution curve of covered ground, look at animated SVGs.

## Looping a Local LLM into Your Actions with WebSockets

Anyhow, let's not digress too much. This writing is my machete'ing my way into
the noosphere, I hope. I got some awesome guidance from o1-preview yesterday,
and I can probably forge ahead using that almost verbatim. But there's another
simpler level I want to peel away first that makes you really viscerally and
emotionally ***feel*** what I'm going for about looping a local LLM in on your
actions, and that's it demonstrating to you it sees everything you do in the
product. I originally programmed it that way, but had to rip out those bits when
I did the object oriented base class so that plugins could inherit from it. All
that OO stuff was new enough to me that I didn't want to complicate it with all
sorts of message passing. It's time to pass those messages. Loosely coupled
components must readily chat with each other through WebSockets, whether or not
the further usage is implemented. All that user-action stuff at least goes into
an ongoing chat context window. Yup, welcome to anti-pattern number one. In most
other regards, this new web framework is merely an uber-simple ajax.ified Django
or Ruby on Rails. But now, it's something fundamentally different.

## Overcoming Soft-Skill Challenges in the Workplace

Life is all about rising up to challenges. That OO challenge was on the
technical side, but I've got a ton of similar little challenges on the
soft-skills side, namely account management stuff. Mikey no likey calendly. What
I may call administrative overhead, office document jockeying, and general
paperwork, many would consider the relationship with the customer and the
lifeblood of the company. I spent a lot of time around Commodore engineers and
picked up bad habits. By the way, the Commodore 64x has been re-issued, and it's
AWESOME!


