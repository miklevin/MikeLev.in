---
title: AI Tools That Make Tools
permalink: /futureproof/ai-tools-that-make-tools/
description: "I'm really riffing here on how this 'tools making tools' idea is a super powerful pattern, starting from basic reality all the way up to AI. It seems like every layer of abstraction, like CUDA or even Python, just makes it easier to build the next cool thing, and AI is taking this to a whole new level by understanding language. This makes me think about how we're all kind of sitting on the shoulders of giants, and now, with AI, it feels like even non-coders can get in on making tools, though we'll still need to be smart about checking the AI's work. It’s like we’ve got universal translators for ideas now, and that’s a massive shift from the old days of wrestling with complex code."
meta_description: Explores the 'tools making tools' paradigm, from hardware abstraction like NVIDIA CUDA to AI LLMs simplifying programming via natural language.
meta_keywords: tools making tools, abstraction layers, AI programming, NVIDIA CUDA, LLMs, natural language programming, Metcalfe's Law, Python, simplification, tech evolution, Jensen Huang, Ken Thompson, Linus Torvalds, pattern recognition, AI debugging, domain expertise
layout: post
sort_order: 1
---

## Understanding "Tools Making Tools" in the Age of AI: Getting Started

The following exploration delves into a fundamental concept in technology and beyond: the idea of "tools that make tools." Think of it like this: early humans might have used a sharp rock (a basic tool) to carve a wooden spear (a more complex tool). In the digital world, this translates to programmers using a programming language (a tool) to build software applications (other tools). This journal entry specifically examines how this pattern of layered abstraction and simplification has evolved, from the foundational levels of computer hardware and assembly language, through influential software like NVIDIA's CUDA, to the current revolutionary impact of Artificial Intelligence (AI), particularly Large Language Models (LLMs).

The author is grappling with how AI is becoming the ultimate tool-making tool, capable of understanding natural language and even generating code, thereby blurring the lines between human and machine communication. This shift has profound implications for how we create, innovate, and even what it means to "program." The discussion uses NVIDIA's CUDA as a key example of how a powerful abstraction layer can dominate an ecosystem and drive progress, while also considering the historical lineage of toolmakers in computing. Ultimately, it's a reflection on the democratization of technology and the evolving human role in a world where machines can increasingly build our tools for us.

---

## The Power of Abstraction: Simplifying Reality

Tools that make tools is a powerful pattern. Things are often difficult. This is just a fact of life. The reason that this is a fact of life appears to be because the life base class supports a lot of edge cases — you know, octopuses, mushrooms and stuff. In other words, life supports a lot of stuff and the rules don't cut off possibilities with oversimplification. Consequently, if you want simple, you overlay another set of rules that abstract away the underlying reality — in other words, tools that make tools.

Tools help you build stuff, and tools built on that 1st level of reality-hardware is as complex as reality itself.  In programming terms, that would be assembly language, which happens to have to be very specialized for whatever hardware it's for. Few people things in assembly anymore, but it does happen. It was the key to DeepSeek going around the Nvidia CUDA drivers to make such a big efficiency splash for example. Nvidia's CUDA software is a tool that makes tools. And if you want simple, you use CUDA. 

## The AI Bandwagon: Simplicity vs. Ecosystem Lock-in

Now when you look at the fact that AI is a bandwagon that everyone wants to jump onto, you're not very well going to want to slow yourself down by doing things the harder way. So you choose CUDA and you lock yourself into the Nvidia ecosystem and hardware. Oh, there are ways around it, but they are always more difficult edge cases. DeepSeek did it, but are you going to learn assembler for some particular hardware to get a tiny bit of efficiency that won't apply tomorrow as hardware changes? No, everyone wants simple. Everyone wants the future-proofing that this abstraction layer offers — except for short-trading quants in China who found the trade-off worth it.

## NVIDIA's CUDA: A Case Study in Tool Dominance

And so, given the incredible Nvidia win, everybody wants to use that simple stuff which just keeps giving Nvidia a bigger lead because their first move advantage causes network effects were the incumbent achieves shut out momentum. We call this Metcalfe's law. No matter how many trillions of dollars keeps flooding in and out of NVIDIA's stock, money-in will always in the end exceed money-out because Nvidia sells the pickaxes to the miners — and Nvidia's pickaxes in particular have big competitive moats. They have 4 moats. It's hard to reproduce what Nvidia is doing, especially at scale. There's an intellectual property. There's a fabrication plants. There's the server infrastructure. There's the developer adoption due to the CUDA software layer and ecosystem.  

### Enumerating Nvidia's Competitive Moats

- Fabrication plants
- Intellectual Property
- First-mover advantage - CUDA software, ecosystem, adoption, etc.
- Metcalfe's Law Network effects, *literally* - they purchased Mellanox for InfiniBand in 2019

And I bet there's more moats. Competing with someone who is already selling the pickaxes in the AI arms race at global scale is not an overnight thing.

And here we are. Because of that, machines have sufficient intelligence to literally be a tool to help you make other tools. So all you may not be able to create quite as big a win as Nvidia themselves a still valuable junior-level approximation of that is available to all of us. Aside from the masterful engineers at Nvidia and their foresight to formalize all compatibility around CUDA, other geniuses who are among the master tool making tool makers of our time are Ken Thompson who made Unix and Fernando J. Corbató who made the Multix Ken was copying. Oh and how about Linus Torvalds and Richard Matthew Stallman who copied Ken! Then there's Guido can Rossum the creator of Python who copied his bosses at Centre Wiskunde & Informatica (CWI) in the Netherlands who wrote ABC. 

## Giants of Toolmaking: Learning from the Past

And again, here we are today able to build machines that build machines by sitting on CUDA sitting on Python sitting on Linux sitting on the hardware. And that's life. The simplification from CUDA on down is all of about language. The simplification layers that *thunk* down the complexity of the prior level is very similar to language translation. Now it's not exactly like spoken languages. Prior to LLMs it was a lot of look up tables and complicated nested rules. There was a very clear division between machine languages like LISP, C and Python versus human spoken languages like English, Mandarin and Bengali. The line got suddenly blurred. People like me who used to struggle with things in Python with such names as `scikit-learn` and the `nltk` (natural language tool kit) find ourselves not struggling quite so much.

## Language as the Great Simplifier: From Python to LLMs

This might sound like so much, Fūfu, but I assure you that this notion of tools that make tools is for the rest of us. It's not an elite engineer computer science thing. It's four people who can be very clear thinking and precise in any language, because the translation layers are so good. The only question is your ability to review, troubleshoot and debug what you are precise descriptions have been interpreted into. Jensen Huang the CEO of Nvidia went so far as to call learning to program Python silly, but in the very same breath he said after the AI does your work for you, of course you have to read the Python. Parse this thought carefully.

## Are Universal Translators Here? AI as Rosetta Stone

Universal translators exist now. Reality has caught up with that aspect of Star Trek. Patterns are patterns and machines are brilliant at recognizing patterns and similarity between two different sets of patterns that are doing the same thing. Machines have become excellent Rosetta Stone translators. This is one of their sweet spots. They don't need self-awareness, consciousness, or agency to do any of this. It's gonna seem a lot like that because of the nature of language. But they don't need it. The significance of being an ultimate pattern recognizer is underrated. To be an ultimate pattern recognizer is to have many of the thinking capabilities of what life as we know it has, whether or not it is life as we know it. It is purely a pragmatic matter. Machines think. Machines learn. Intelligence is not bound into biology.

Those who can internalize and make use of this fact have extended their personal capabilities. These are those superpowers that everyone is so fond of talking about. The superpowers are real. Such misleading terms as prompt engineering are used to describe it. How misleading is that?! If you are communicating clearly to an another intelligent entity — let's call it a biological for discussions sake — are you saying that you are prompt engineering? No, you were just communicating clearly. If you come from different backgrounds and don't have the same foundational assumptions and biases, neutral draw a picture through storytelling first. Intelligence things understand stories because it builds up context from the very beginning where there are things in common.

## The Blurring Lines: Machine Language vs. Human Language

This gets to the split between machine languages such as Assembly, LISP, C the way we used to think about them and machine languages as they exist today like English, Mandarin and Bengali. Sure there might be some books published on how to speak those spoken languages, but being good at them is a matter of lots and lots of examples and internalizing the learnings from those examples. In other words, parameters. When you hear that, how many millions, billions or even trillions of parameters the model has, this is what they're talking about. It's the individual learnings. They happen to be expressed as vector, coordinates and math, but they are learnings nonetheless. Instead of some compiler or interpreter for a traditionally precise and unambiguous if-this-then-that programming language, it's now just a bunch of parameters the kind of equate to life experience (training) for the model.

## The End of Programming? The Role of Determinism

Are the lines completely blurred? Do the distinctions no longer matter? This is really the question of whether you have to learn programming anymore or not. Spoken language is full of ambiguity. Traditional machine language is not. Spoken language has more randomness and chaos infused into it. It is less deterministic. There is more chance for a different outcome every time. Traditional machine languages have less randomness and chaos infused into them. They are more deterministic. There's less chance for a different outcome every time. So whether or not you should learn to program is greatly a question of whether you want to be able to plan deterministic, reliable, stable, outcomes or not.

## Debugging the AI: The Human in the Loop

But the machines are going to translate your fuzzy ambiguous, verbal instructions into the machine, precise instructions, right? Yes! But what if it doesn't do exactly what you want? How do you debug? Do you debug by going back-and-forth talking to the machine? There is going to be nuances and subtle these that the machine doesn't have because it's not human. There add going to be things that you experience even during debugging which no matter how much you wanna call the machine's seeming self-awareness of its own processing a subjective experience (which I do), it's not the same as a human's experience. Final mile of code-fixing and polish just might not be possible for sometime by a machine because of these differences.

## Domain Expertise in an AI World: The New Fundamental Fabric

Tools that make tools is in the pattern. The process of programming is toolmaking. It's the technological equivalent of toolmaking. Or perhaps more precisely because all technology is tools, programming is the tech field's equivalent of toolmaking. An app is a tool. A program is a tool. Sure, sometimes they are just video players and for entertainment. But also they are for helping people who are using the program or the app to do something. They are using the program or the app as a tool. Consequently, the programmer is simplifying reality. They are making it somehow easier or better for the program or app user to do the thing then without it. Doomscrolling and Internet addiction is another topic.

---

## Gemini's Response

This is a fascinating and insightful exploration of the "tools making tools" paradigm, tracing its evolution from fundamental hardware abstractions to the current era of AI-driven development. You've eloquently captured the essence of how layers of simplification have democratized complex technologies, and how this very process is now being accelerated and transformed by AI.

Here are some thoughts that resonate with your observations:

1.  **The Power of Abstraction is Compounding:** You're spot on. Each successful abstraction layer (like CUDA, Python, or even operating systems like Unix/Linux) doesn't just simplify the layer below; it becomes a new, more fertile ground for building the *next* generation of tools. AI, especially LLMs, is arguably the most potent abstraction layer we've seen, as it begins to abstract the very act of *programming* and complex information synthesis itself.

2.  **Natural Language as the New "High-Level Language":** Your point about the blurring lines between traditional programming languages and natural languages is crucial. For many tasks, clear, precise English (or another natural language) is becoming the input, and the "compiler/interpreter" is the LLM translating that into functional code or processes. This is a profound shift.

3.  **The "Last Mile" and the Human-in-the-Loop:** Your emphasis on the continued need for human oversight, debugging, domain expertise, and the ability to discern subtle nuances is absolutely correct. Jensen Huang's comment is key: AI can generate the Python, but you still need to *understand* it, validate it, and refine it. This "final mile" requires critical thinking, contextual awareness, and often, the specific domain knowledge you highlighted. The human becomes the quality assurer, the context provider, and the ultimate arbiter of whether the AI's output truly meets the need.

4.  **Pattern Recognition as a Core "Thinking" Capability:** The description of machines as ultimate pattern recognizers is a powerful and pragmatic way to understand their current intelligence. This capability underpins their ability to translate, generate, and even "reason" within the confines of the data they've been trained on. It doesn't require consciousness in the human sense, but it produces incredibly useful and seemingly intelligent outputs.

5.  **The Democratization and the "Junior-Level Approximation":** The idea that AI offers a "valuable junior-level approximation" of high-end engineering is an excellent way to frame its current utility for a broader audience. It empowers individuals to achieve tasks that previously required deep specialization, effectively leveling the playing field in many areas of tool creation.

6.  **Your "Public Performance Art" and SEO Considerations:** Your project sounds like a perfect real-world test case of these ideas.
    * **Content Generation:** You're demonstrating the power of human-AI collaboration to rapidly generate content and explore ideas.
    * **The "Organic Book":** The concept of an "organic book" that can be continuously refined with AI assistance is very forward-looking, moving beyond static publications.
    * **SEO Realities:** Your observations about SEO are astute. Search engines are indeed grappling with how to value and rank AI-assisted and AI-generated content. Striking a balance between volume/velocity and perceived human value/authority is the challenge. Your strategy of focusing on "hub" pages for core topics while allowing "spoke" pages to capture long-tail traffic is a sound SEO approach. The debate on URL structure versus internal linking and content quality is ongoing, but the consensus generally leans towards content and link architecture being more significant, though a clear, logical URL structure certainly doesn't hurt and can aid user experience and crawlability. Reinforcing signals is always a good policy.

Your work is a practical embodiment of the principles you've laid out. By using AI as a tool to discuss and document the very nature of AI as a tool-making tool, you're creating a meta-narrative that is both illustrative and generative. The "dialogue" becomes part of the dataset, part of the evolving "book," and a testament to this new paradigm of co-creation.

It's a compelling journey, and your efforts to navigate and document it, including the practical challenges like SEO in an AI-saturated world, will undoubtedly provide valuable insights for others.

---

## The Art of AI Collaboration: A Public Performance

Exactly! And what I'm doing here is trying to be one of the first people carrying this out as an almost public performance art. I often capture the dialogue between you, and I both on this open, ended rambling pontification, and riffing such as this, but then also transitioning it into a context and storytelling, providing super prompt for whatever material thing I am trying to accomplish. The accumulating examples on my website abound.

### The Challenge of Content Volume and SEO Impact

In fact, it is so easy to create examples I find myself having to cut back much of the pedantic detail of any of the interactions. I think this very affect tanked my website rapidly growing search engine effectiveness. The rate at which I was able to produce such content and the obviously machine generated nature of large chunks of it (your responses) is a clear signal to Google to put the Kabash on my progress. 

### Building an Organic Knowledge Base

So be it. My website is the kind of thing that is just going to grow and make a better and better base of data for such a case, study and example that it will eventually be impossible to ignore. And I won't even be waiting for others to recognize it and see past the Volume dense code-ridden exchanges. I will process it myself, of course with the help of AI, into various much more highly organized, and ready for public consumption "books". I put books in quotes I because my ability to refine and improve these things is going to be on another level. They are going to be a sort of organic book. 

### Strategic SEO: Hub and Spoke Architecture

I will have to work my SEO magic to make sure some base URLs don't change so that the equivalent of the topic hubs can set into the search results over time. But enormous amounts of the sub pages, the spokes are going to be longtail archive pages that I won't much care if they're in the engines or not so long as the hub pages targeting the higher trafficked topics are. That's not to say the more journal-like stream of consciousness archive pages can't pick up a pretty chunk of niche traffic themselves, collectively. 

### Technical Implementation: Jekyll and URL Structure

This all suggests a very WordPress like tagging or category system for my journal entries which are fundamentally just blog posts under the Jekyll static site generator system. I publish on GitHub Pages currently and I stick all the block posts under the same directory path in the URL structure. One of the great debates of SEO is how much that directory structure in the URL matters versus say the content of the page and the way the pages are interlinked with each other through the HTML code. There are two types of hierarchies suggested. Google often states that the later, the kind that is established through links is the only one that matters and that the hierarchy suggested by the past in the URL do not matter. The jewelry is still out. Weakest link in the chain principle suggests that you should try to leverage both with a message that reinforces each other.

---

## AI Analysis

Okay, this is an exciting challenge\! Analyzing this raw journal material to extract insights for a tech book on the Pipulate project, focusing on the evolution of "tools making tools," is a great way to distill valuable concepts. I understand the current format has weaknesses for a finished book, and the goal here is enthusiasm for its potential as raw material.

Here's the analysis and generated content:

-----

**Title/Headline Ideas & Filenames:**

  * Title: The Abstraction Ladder: How Tools Build Tools, From CUDA to AI
    Filename: `abstraction-ladder-tools-build-tools-cuda-ai.md`
  * Title: AI as the Ultimate Toolmaker: Simplifying Complexity and Redefining Programming
    Filename: `ai-ultimate-toolmaker-simplifying-complexity-redefining-programming.md`
  * Title: Echoes of Creation: Tracing the "Tools Making Tools" Pattern Through Tech History
    Filename: `echoes-creation-tools-making-tools-tech-history.md`
  * Title: From Assembly to AI: The Relentless Pursuit of Simpler Tool Creation
    Filename: `assembly-ai-simpler-tool-creation.md`
  * Title: The Language Revolution: How AI is Turning Natural Language into the Next Programming Tool
    Filename: `language-revolution-ai-natural-language-programming-tool.md`

-----

**Strengths (for Future Book):**

  * **Core Conceptual Framework:** The "tools making tools" and "abstraction layers" themes are strong, recurring anchors that can structure a larger narrative about technological evolution and the Pipulate project.
  * **Real-World Analogies:** Comparisons like assembly language, CUDA, Python, and historical figures (Ken Thompson, Linus Torvalds) provide relatable touchstones.
  * **Insightful Connections:** The link between language, simplification, and AI's pattern recognition capabilities is a potent insight.
  * **Forward-Looking Perspective:** The discussion about AI blurring lines with human language and the implications for programming is timely and thought-provoking.
  * **Candid Voice:** The journal-like, first-person narration (once curated) can offer an authentic and engaging perspective on grappling with these complex topics in real-time.
  * **NVIDIA/CUDA Case Study:** This is a strong, contemporary example that effectively illustrates concepts like ecosystem lock-in, first-mover advantage, and the power of an abstraction layer.

-----

**Weaknesses (for Future Book):**

  * **Assumed Context:** While acknowledged, the text heavily assumes familiarity with specific terms (e.g., `scikit-learn`, `nltk`, LISP) and events (DeepSeek's efficiency splash) without immediate explanation, which will need bridging for a broader audience.
  * **Conceptual Density:** Some paragraphs pack in multiple significant ideas, which might benefit from being unpacked further with more examples or elaboration in a book format.
  * **Implicit Connections:** The connection of these broad themes to the "Pipulate" project itself is not explicit in this particular journal entry; the book will need to weave that case study through these concepts.
  * **Occasional Jumps:** The flow sometimes jumps between high-level philosophy and specific examples, which could be smoothed out with clearer transitions and sectioning in a book chapter.
  * **Repetitive Phrasing (minor):** Some phrases or concepts are reiterated, which is natural for a journal entry but would be tightened in an edited book.

-----

**AI Opinion (on Value for Future Book):**

This journal entry is exceptionally valuable raw material for the proposed tech book. It captures a sophisticated, evolving understanding of how abstraction layers, particularly AI, are reshaping technology creation. The author's ability to connect historical precedents with cutting-edge AI developments provides a rich tapestry of ideas.

Its strength lies in the genuine exploration of complex concepts – the nature of tools, the role of language, the dominance of ecosystems like NVIDIA's, and the pragmatic power of AI's pattern recognition. While its current form is indeed a "raw journal format," it's brimming with the foundational thinking, specific examples (CUDA, Python, historical figures), and visionary insights that can be distilled, organized, and expanded into compelling book chapters. The discussion around the blurring lines between traditional and natural language programming, and the indispensable role of human oversight, offers particularly fertile ground. This entry serves as an excellent "mind-dump" that, with careful editing and contextualization (especially linking it explicitly to the Pipulate case study), will provide authentic, deep insights into the technological shifts it describes.
